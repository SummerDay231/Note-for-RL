## Q-learning

### 行为准则

在什么状态下采取什么行动会有什么后果，根据这个决定下一步行动

### Q-learning 决策

假设行为准则已经学习好了：

我们处于状态s1（写作业），我有两个行为a1（看电视）和a2（写作业）

根据我的经验（行为准则），a2比a1的潜在奖励高，我们选择a2作为下一个行为，到达状态s2（继续写作业）

### Q-learning更新

|      | a1   | a2   |
| ---- | ---- | ---- |
| s1   | -2   | 1    |
| s2   | -4   | 2    |

根据Q表的估计，s1中，a2值比较大，采取a2，到达s2

**这时候开始更新Q表**：我们并没有采取任何行动，而是*想象*自己在s2上采取每种行动，分别看看两种行为哪一个的Q值大，这里Q(s2, a2)的值比Q(s2, a1)的值大，所以我们把这个最大的值乘以一个衰减值加上到达s2所获取的奖励R，将这个作为现实中Q(s1, a2)的值，用估计值和现实值之间的差距乘以学习率去更新估计值

s2的决策要到下一步再进行更新

### Q-learning整体算法

```
初始化Q(s, a)
	每一阶段重复：
		初始化s
		对每一步重复：
			用某些策略选择s下的a
			采取行动a，得到奖励r，转移到状态s'
			更新上一步的行为准则：Q(s, a) = Q(s, a) + alpha[r+gamma*maxQ(s',a')-Q(s,a)]
			s变成s'
		直到s为终止状态
```

$\epsilon$ greedy：有多大概率选择最优值行为，否则随机行为

### Q-learning中的gamma

gamma越接近1，对未来的利益考虑得越多；越接近0，越注重眼前的利益

